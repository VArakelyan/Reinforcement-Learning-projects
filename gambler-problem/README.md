### ðŸŽ° **Project 5: Gamblerâ€™s Problem â€“ Value Iteration Approach**

- Simulated the **Gamblerâ€™s Problem** as a finite MDP, where states represent capital values from 0 to 100.
- Implemented the **Value Iteration** algorithm to iteratively compute the optimal state-value function.
- Defined actions as all possible stakes bounded by the current capital and distance to the goal.
- Used a biased coin flip (p = 0.4 for heads) to define stochastic transitions.
- Designed the reward structure to give **+1 only when reaching the goal (capital = 100)**; all other transitions yield 0.
- Extracted the optimal policy by choosing the stake at each state that maximizes expected value.
- Visualized:
  - ðŸ“ˆ Value function updates across sweeps
  - ðŸ§  Final optimal policy (stakes per capital)

ðŸ“˜ *Based on Sutton & Barto, Reinforcement Learning: An Introduction â€“ Figure 4.3*
